{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d0084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN algorithm\n",
    "# description: In CNN we follow muliple steps as \n",
    "# convolutional layers, where kernel interact with the image and an activation finction will act on it to make it non-linear\n",
    "# max or avg pooling where dimesions are reduced\n",
    "# falttening of the tensor into vector\n",
    "# fully connnected layers, which connects all neurons from flattening layer to the output\n",
    "# aprt form this there are other terms also thta will be included like paddig, stride etc\n",
    "\n",
    "# important: there are pre trained models for CNN like MobileNet and VGG16. these can be used in future integrating with other methods     \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Steps in the Code:\n",
    "\n",
    "1.Create and normalize the grayscale image.\n",
    "2. Define the kernel and add padding to the image.\n",
    "3.Perform convolution with ReLU activation.\n",
    "4. Apply max pooling to reduce spatial dimensions.\n",
    "5. Flatten the pooled feature map into a 1D vector.\n",
    "6. Pass the flattened vector through the fully connected layer.\n",
    "7. Apply softmax activation for classification.\n",
    "8. Apply dropout for regularization during training.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# importing the required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image  # For loading and preprocessing the image\n",
    "import matplotlib.pyplot as plt  # For displaying the image\n",
    "import tensorflow as tf  # For building the CNN model\n",
    "import tensorflow_datasets as tfds  # For loading the dataset\n",
    "\n",
    "\n",
    "# we need a dataset to tarin our CNN model\n",
    "# we use tensorflow dataset\n",
    "# in case you have your own data set, save the dataset in diffrent folders \n",
    "# the different folder belong to different classes need to be identified classes for example, class1 tulip, class2, roses and class3sunflower etc\n",
    "\n",
    "\"\"\" \n",
    "data pipeline should be like this\n",
    "dataset/\n",
    "    tulip/          # Folder containing all tulip images\n",
    "        tulip1.jpg\n",
    "        tulip2.jpg\n",
    "        ...\n",
    "    rose/           # Folder containing all rose images\n",
    "        rose1.jpg\n",
    "        rose2.jpg\n",
    "        ...\n",
    "    sunflower/      # Folder containing all sunflower images\n",
    "        sunflower1.jpg\n",
    "        sunflower2.jpg\n",
    "        ...\n",
    "\n",
    "in this dataset downloaded form tf the data is \n",
    "daisy: label = 0\n",
    "dandelion: label = 1\n",
    "roses: label = 2\n",
    "sunflowers: label = 3\n",
    "tulips: label = 4\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Specify a custom path for the dataset\n",
    "custom_path = r\"flower_data\"  # Replace with your desired folder path\n",
    "\n",
    "# Load the tf_flowers dataset and specify the custom path\n",
    "dataset, info = tfds.load('tf_flowers', as_supervised=True, with_info=True, data_dir=custom_path)\n",
    "\n",
    "\n",
    "class_names = info.features['label'].names\n",
    "print(\"Class Names:\", class_names)\n",
    "\n",
    "# Take a few examples from the dataset\n",
    "for image, label in dataset['train'].take(10):  # Display 10 images\n",
    "    plt.imshow(image.numpy())  # Convert Tensor to NumPy array for visualization\n",
    "    plt.title(f\"Label: {label.numpy()}\")  # Display the label\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# its important to keep the class names and defne later for output to understand the model\n",
    "\n",
    "\n",
    "\n",
    "# # Split the dataset into training and validation sets\n",
    "# The dataset is currently not shuffled before splitting. It is importent to shuffle. \n",
    "# If the data is ordered by class, this can lead to biased splits (e.g., all tulips in the training set, all roses in the validation set\n",
    "\n",
    "shuffled_dataset = dataset['train'].shuffle(1000)\n",
    "train_dataset = shuffled_dataset.take(3500) # First 3500 examples for training\n",
    "val_dataset = shuffled_dataset.skip(3500)      ## Remaining examples for validation\n",
    "\n",
    "\n",
    "# Preprocess the data: Resize and normalize images\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.resize(image, (32, 32))  # Resize to 32x32\n",
    "    image = image / 255.0                     # Normalize to [0, 1]\n",
    "    return image, label\n",
    "\n",
    "\n",
    "\n",
    "# Apply preprocessing function to the training and validation datasets\n",
    "# this is donr in batch to make it faster\n",
    "# Note: The batch size is set to 32, and the dataset is shuffled with a buffer size of 1000.\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess).batch(32).shuffle(1000)\n",
    "val_dataset = val_dataset.map(preprocess).batch(32)\n",
    "\n",
    "\n",
    "\n",
    "# Print an example batch\n",
    "for images, labels in train_dataset.take(1):\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    print(\"Label batch shape:\", labels.shape)\n",
    "\n",
    "\n",
    "\n",
    "# define a CNN model\n",
    "# this includes: input, con layers, pooling, flattening, fully connected ayers, and output layer\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        # Convolutional Layer 1\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape), # 1st hidden layer (remember only conv is hidden layer)\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),  # Pooling Layer 1\n",
    "        \n",
    "        # Convolutional Layer 2\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),      # second hiddenlayer\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),  # Pooling Layer 2\n",
    "        \n",
    "        # Flatten the feature maps\n",
    "        tf.keras.layers.Flatten(),\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        tf.keras.layers.Dense(128, activation='relu'),      ## 3rd hidden layer (fully connected)\n",
    "        tf.keras.layers.Dropout(0.5),  # 50 % Dropout for regularization. 50% of the neurons in the layer will be randomly deactivated (set to zero)\n",
    "        \n",
    "        # Output Layer\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')  # Use softmax for multi-class classification\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Pooling, dropout, and flattening layers are not considered hidden layers\n",
    "# upgrade: increase the no of hiddenlayer (only conv)\n",
    "# Define the input shape and number of classes\n",
    "input_shape = (32, 32, 3)  # Images are resized to 32x32 with 3 channels (RGB)\n",
    "num_classes = 5  # Number of flower classes (e.g., tulip, rose, etc.)\n",
    "\n",
    "\n",
    "# Create the CNN model\n",
    "model = create_cnn_model(input_shape, num_classes)\n",
    "\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# one can use learning rate scheduler to adjust the learning rate during training\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "\n",
    "\n",
    "# define the optimizer with desired learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer= optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# other type of otimsers that can be used: SGD (Stochastic Gradient Descent), RMSprop, Adagrad\n",
    "# other metrics: Precision, Recall, and F1-Score (useful for imbalanced datasets) and Mean Squared Error (for regression tasks).\n",
    "# Train the model\n",
    "history = model.fit(train_dataset, epochs=30, validation_data=val_dataset)\n",
    "# increase the no of epochs for training\n",
    "\n",
    "\n",
    "# saving the model\n",
    "model.save('cnn_model.h5')\n",
    "\n",
    "\n",
    "# Plotting the training and validation accuracy and loss over epochs\n",
    "# Access the training and validation accuracy\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "# Access the training and validation loss\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot the accuracy\n",
    "plt.plot(train_acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.show()\n",
    "\n",
    "# Plot the loss\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss over Epochs')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# test the model on new images\n",
    "\n",
    "test_folder = r\"flower_data\\test\"  # Path to your test folder\n",
    "\n",
    "# Class names for the dataset (e.g., tf_flowers)\n",
    "# make sure the class names are same as the one used in training\n",
    "\n",
    "class_names = ['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']\n",
    "\n",
    "# Loop through each image in the folder\n",
    "for file_name in os.listdir(test_folder):\n",
    "    file_path = os.path.join(test_folder, file_name)\n",
    "    \n",
    "    # Check if the file is an image\n",
    "    if file_name.endswith(('.jpg', '.png', '.jpeg', '.jif')):\n",
    "        try:\n",
    "            # Step 1: Load and preprocess the image using PIL\n",
    "            image = Image.open(file_path)  # Open the image\n",
    "            image = image.resize((32, 32))  # Resize to 32x32\n",
    "            image_array = np.array(image) / 255.0  # Convert to numpy array and normalize to [0, 1]\n",
    "            image_batch = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "            \n",
    "            # Step 2: Predict the class probabilities\n",
    "            predictions = model.predict(image_batch)\n",
    "            \n",
    "            # Step 3: Get the predicted class\n",
    "            predicted_class = np.argmax(predictions[0])  # Get the class index with the highest probability\n",
    "            predicted_class_name = class_names[predicted_class]\n",
    "            \n",
    "            # Step 4: Display the image with its prediction\n",
    "            plt.imshow(image)  # Display the image\n",
    "            plt.axis('off')  # Turn off axis\n",
    "            plt.title(f\"Predicted: {predicted_class_name}\")  # Add the prediction as the title\n",
    "            plt.show()  # Show the plot\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
